{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-08T01:22:07-06:00\n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.2.0\n",
      "\n",
      "compiler   : GCC 4.8.2 20140120 (Red Hat 4.8.2-15)\n",
      "system     : Linux\n",
      "release    : 4.15.0-43-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(datos.data, columns=datos.feature_names)\n",
    "boston[\"objetivo\"] = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>objetivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  objetivo  \n",
       "0     15.3  396.90   4.98      24.0  \n",
       "1     17.8  396.90   9.14      21.6  \n",
       "2     17.8  392.83   4.03      34.7  \n",
       "3     18.7  394.63   2.94      33.4  \n",
       "4     18.7  396.90   5.33      36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INCISO**\n",
    "\n",
    "Hasta ahora hemos usado la magia de jupyter notebook `?` y `??` para ver cual es la documentacion de una clase o función. Jupyter lo unico que hace es leer el docstring de dichos objetos. El docstring de un objeto en python es el string delimitado por `\"\"\"` que se define justo después del nombre de dicho objeto, y tiene como objetivo el documentar el uso del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si creamos la clase `ClaseTest` con un docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaseTest():\n",
    "    \"\"\"Éste es el docstring\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos usar la magia de jupyter para que nos imprima el docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClaseTest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos imprimir directamente el atributo `__doc__` de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éste es el docstring\n"
     ]
    }
   ],
   "source": [
    "print(ClaseTest.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a evaluar los algoritmos que conocemos hasta ahora y compararlos con los distintos algoritmos de ensamblado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_cv(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return rmse(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.662138107509335,\n",
       " 'elasticnet': 5.261050219131485,\n",
       " 'lasso': 5.464920344699857,\n",
       " 'ridge': 5.099645204961243}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "estimador_arbol = DecisionTreeRegressor()\n",
    "\n",
    "error_cv = cross_val_score(estimador_arbol, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"arbol\"] = error_cv\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "\n",
    "estimador_elnet = ElasticNet()\n",
    "\n",
    "resultados[\"elasticnet\"] = cross_val_score(estimador_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "\n",
    "estimador_lasso = Lasso()\n",
    "estimador_ridge = Ridge()\n",
    "\n",
    "\n",
    "resultados[\"lasso\"] = cross_val_score(estimador_lasso, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"ridge\"] = cross_val_score(estimador_ridge, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Bagging (Bootstrap aggregating) funcionan entrenando varios estimadores base y cambiando los datos de entrenamiento para cada uno. En sklearn los algoritmos de ensamblado de modelos se encuentran en el submódulo `sklearn.ensemble`. En cuanto a Bagging, sklearn tiene una versión para problemas de regresión (`BaggingRegressor`) y otra para problemas de clasificación (`BaggingClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bagging regressor.\n",
      "\n",
      "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "    regressors each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions (either by voting or by averaging)\n",
      "    to form a final prediction. Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "    tree), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "    This algorithm encompasses several works from the literature. When random\n",
      "    subsets of the dataset are drawn as random subsets of the samples, then\n",
      "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "    of the dataset are drawn as random subsets of the features, then the method\n",
      "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "    on subsets of both samples and features, then the method is known as\n",
      "    Random Patches [4]_.\n",
      "\n",
      "    Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, optional (default=None)\n",
      "        The base estimator to fit on random subsets of the dataset.\n",
      "        If None, then the base estimator is a decision tree.\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "            - If int, then draw `max_samples` samples.\n",
      "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    max_features : int or float, optional (default=1.0)\n",
      "        The number of features to draw from X to train each base estimator.\n",
      "            - If int, then draw `max_features` features.\n",
      "            - If float, then draw `max_features * X.shape[1]` features.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether samples are drawn with replacement.\n",
      "\n",
      "    bootstrap_features : boolean, optional (default=False)\n",
      "        Whether features are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to True, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit\n",
      "        a whole new ensemble.\n",
      "\n",
      "    n_jobs : int, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of estimators\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by a boolean mask.\n",
      "\n",
      "    estimators_features_ : list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_prediction_` might contain NaN.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "           1996.\n",
      "\n",
      "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "           1998.\n",
      "\n",
      "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BaggingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1718899734264605"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_10 = BaggingRegressor(n_estimators=10)\n",
    "error_cv = cross_val_score(estimador_bagging_10, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_10\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentar el número de estimadores base es una forma limitada pero sencilla de mejorar el funcionamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1851563483775065"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_100 = BaggingRegressor(n_estimators=100)\n",
    "error_cv = cross_val_score(estimador_bagging_100, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaggingRegressor` utiliza árboles de decisión como estimador base por defecto, sin embargo podemos utilizar uno distinto mediante el parámetro `base_estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.262190339491701"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ElasticNet())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_elnet\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su momento vimos que existe un tipo de arbol de decision completamente aleatorio (Extremely Randomized Trees) que deciden la particion en cada nodo al azar. Vemos que al agrupar muchos de estos estimadores que son débiles (aun que mejores que tirar una moneda al azar, ya que un árbol de decision aleatorio aun asi aprende a separar los elementos), la varianza general se reduce ya que la que aporta un arbol se complementa con la del de al lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840832188483701"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor\n",
    "\n",
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ExtraTreeRegressor())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_extra_arbol\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de boosting intentan mejorar los estimadores base asignando pesos en funcion de su funcionamiento individual. El algoritmo clásico de boosting es `AdaBoost`, que se encuentra en sklearn como `AdaBoostRegressor` y `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AdaBoost regressor.\n",
      "\n",
      "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "    regressor on the original dataset and then fits additional copies of the\n",
      "    regressor on the same dataset but where the weights of instances are\n",
      "    adjusted according to the error of the current prediction. As such,\n",
      "    subsequent regressors focus more on difficult cases.\n",
      "\n",
      "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "\n",
      "    Read more in the :ref:`User Guide <adaboost>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object, optional (default=DecisionTreeRegressor)\n",
      "        The base estimator from which the boosted ensemble is built.\n",
      "        Support for sample weighting is required.\n",
      "\n",
      "    n_estimators : integer, optional (default=50)\n",
      "        The maximum number of estimators at which boosting is terminated.\n",
      "        In case of perfect fit, the learning procedure is stopped early.\n",
      "\n",
      "    learning_rate : float, optional (default=1.)\n",
      "        Learning rate shrinks the contribution of each regressor by\n",
      "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "        ``n_estimators``.\n",
      "\n",
      "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "        The loss function to use when updating the weights after each\n",
      "        boosting iteration.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of classifiers\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimator_weights_ : array of floats\n",
      "        Weights for each estimator in the boosted ensemble.\n",
      "\n",
      "    estimator_errors_ : array of floats\n",
      "        Regression error for each estimator in the boosted ensemble.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances if supported by the ``base_estimator``.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "           on-Line Learning and an Application to Boosting\", 1995.\n",
      "\n",
      "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(AdaBoostRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.462756793365546"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_adaboost = AdaBoostRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_adaboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"adaboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro algoritmo de Boosting es Gradient Boosting que a cada iteración usa el algoritmo de Descenso de Gradiente (que veremos en el futuro) para a cada iteración , entrenar un estimador nuevo que minimiza la función de error (*loss function*) del modelo.\n",
    "\n",
    "Scikit-learn implementa el algoritmo de (Gradient Boosted Regression Trees), que usa árboles de decisión como estimadores base, en [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) y [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "Gradient Boosting puede usar cualquier funcion de error siempre que sea diferenciable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting for regression.\n",
      "\n",
      "    GB builds an additive model in a forward stage-wise fashion;\n",
      "    it allows for the optimization of arbitrary differentiable loss functions.\n",
      "    In each stage a regression tree is fit on the negative gradient of the\n",
      "    given loss function.\n",
      "\n",
      "    Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "        loss function to be optimized. 'ls' refers to least squares\n",
      "        regression. 'lad' (least absolute deviation) is a highly robust\n",
      "        loss function solely based on order information of the input\n",
      "        variables. 'huber' is a combination of the two. 'quantile'\n",
      "        allows quantile regression (use `alpha` to specify the quantile).\n",
      "\n",
      "    learning_rate : float, optional (default=0.1)\n",
      "        learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "        There is a trade-off between learning_rate and n_estimators.\n",
      "\n",
      "    n_estimators : int (default=100)\n",
      "        The number of boosting stages to perform. Gradient boosting\n",
      "        is fairly robust to over-fitting so a large number usually\n",
      "        results in better performance.\n",
      "\n",
      "    max_depth : integer, optional (default=3)\n",
      "        maximum depth of the individual regression estimators. The maximum\n",
      "        depth limits the number of nodes in the tree. Tune this parameter\n",
      "        for best performance; the best value depends on the interaction\n",
      "        of the input variables.\n",
      "\n",
      "    criterion : string, optional (default=\"friedman_mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"friedman_mse\" for the mean squared error with improvement\n",
      "        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "        the mean absolute error. The default value of \"friedman_mse\" is\n",
      "        generally the best as it can provide a better approximation in\n",
      "        some cases.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a percentage and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node:\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a percentage and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    subsample : float, optional (default=1.0)\n",
      "        The fraction of samples to be used for fitting the individual base\n",
      "        learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "        Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "        Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=None)\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a percentage and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Choosing `max_features < n_features` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_split : float,\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    alpha : float (default=0.9)\n",
      "        The alpha-quantile of the huber loss function and the quantile\n",
      "        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "\n",
      "    init : BaseEstimator, None, optional (default=None)\n",
      "        An estimator object that is used to compute the initial\n",
      "        predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      "        If None it uses ``loss.init_estimator``.\n",
      "\n",
      "    verbose : int, default: 0\n",
      "        Enable verbose output. If 1 then it prints progress and performance\n",
      "        once in a while (the more trees the lower the frequency). If greater\n",
      "        than 1 then it prints progress and performance for every tree.\n",
      "\n",
      "    warm_start : bool, default: False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just erase the\n",
      "        previous solution.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    presort : bool or 'auto', optional (default='auto')\n",
      "        Whether to presort the data to speed up the finding of best splits in\n",
      "        fitting. Auto mode by default will use presorting on dense data and\n",
      "        default to normal sorting on sparse data. Setting presort to true on\n",
      "        sparse data will raise an error.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           optional parameter *presort*.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    feature_importances_ : array, shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    oob_improvement_ : array, shape = [n_estimators]\n",
      "        The improvement in loss (= deviance) on the out-of-bag samples\n",
      "        relative to the previous iteration.\n",
      "        ``oob_improvement_[0]`` is the improvement in\n",
      "        loss of the first stage over the ``init`` estimator.\n",
      "\n",
      "    train_score_ : array, shape = [n_estimators]\n",
      "        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "        model at iteration ``i`` on the in-bag sample.\n",
      "        If ``subsample == 1`` this is the deviance on the training data.\n",
      "\n",
      "    loss_ : LossFunction\n",
      "        The concrete ``LossFunction`` object.\n",
      "\n",
      "    init : BaseEstimator\n",
      "        The estimator that provides the initial predictions.\n",
      "        Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "\n",
      "    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data and\n",
      "    ``max_features=n_features``, if the improvement of the criterion is\n",
      "    identical for several splits enumerated during the search of the best\n",
      "    split. To obtain a deterministic behaviour during fitting,\n",
      "    ``random_state`` has to be fixed.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, RandomForestRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "\n",
      "    J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "\n",
      "    T. Hastie, R. Tibshirani and J. Friedman.\n",
      "    Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(GradientBoostingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9197351753210667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_gradientboost = GradientBoostingRegressor(n_estimators=100, loss='ls')\n",
    "\n",
    "error_cv = cross_val_score(estimador_gradientboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"gradientboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cualquier estimador basado en árboles, `GradientBoostRegressor` nos permite ver la importancia de las variables en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimador_gradientboost.fit(boston[datos.feature_names], boston.objetivo)\n",
    "\n",
    "importancia_variables = estimador_gradientboost.feature_importances_\n",
    "importancia_variables = 100.0 * (importancia_variables / importancia_variables.max())\n",
    "sorted_idx = np.argsort(importancia_variables)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.barh(pos, importancia_variables[sorted_idx], align='center')\n",
    "plt.yticks(pos, datos.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosques Aleatorios (Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Bosques Aleatorios funciona mediante la creación de árboles de decision entrenados en un subgrupo aleatorio de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest regressor.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of classifying\n",
      "    decision trees on various sub-samples of the dataset and use averaging\n",
      "    to improve the predictive accuracy and control over-fitting.\n",
      "    The sub-sample size is always the same as the original\n",
      "    input sample size but the samples are drawn with replacement if\n",
      "    `bootstrap=True` (default).\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"mse\" for the mean squared error, which is equal to variance\n",
      "        reduction as feature selection criterion, and \"mae\" for the mean\n",
      "        absolute error.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a percentage and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a percentage and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node:\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a percentage and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_split : float,\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "\n",
      "    oob_score : bool, optional (default=False)\n",
      "        whether to use out-of-bag samples to estimate\n",
      "        the R^2 on unseen data.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>>\n",
      "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "    >>> regr.fit(X, y)\n",
      "    RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "               max_features='auto', max_leaf_nodes=None,\n",
      "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "               min_samples_leaf=1, min_samples_split=2,\n",
      "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "               oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "    >>> print(regr.feature_importances_)\n",
      "    [ 0.17339552  0.81594114  0.          0.01066333]\n",
      "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "    [-2.50699856]\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, ExtraTreesRegressor\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(RandomForestRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de scikit-learn de RandomForest hace que cada árbol se entrene en base a un dataset del mismo tamaño que el original (con reemplazo si se usa la opción `bootstrap=True`).\n",
    "\n",
    "En cuanto al criterio para evaluar la calidad de la separación de un node de cada árbol base, para la implementación de Regresion, `RandomForestRegressor` usa el error medio cuadrático `mse` por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.161973794510037"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_randomforest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_randomforest, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"randomforest_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.662138107509335,\n",
       " 'elasticnet': 5.261050219131485,\n",
       " 'lasso': 5.464920344699857,\n",
       " 'ridge': 5.099645204961243,\n",
       " 'bagging_arbol_10': 4.1718899734264605,\n",
       " 'bagging_arbol_100': 4.1851563483775065,\n",
       " 'bagging_elnet': 5.262190339491701,\n",
       " 'bagging_extra_arbol': 3.840832188483701,\n",
       " 'adaboost_100': 4.462756793365546,\n",
       " 'gradientboost_100': 3.9197351753210667,\n",
       " 'randomforest_100': 4.161973794510037}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) es un algoritmo de boosting relativamente nuevo que tiene bastante acogida. Es una implementación de Gradient Boosted Trees pero enfocado a datasets grandes.\n",
    "\n",
    "Al ser muy nuevo (el proyecto se creó en 2014 y el paper se publicó en 2016, [éste es el paper](https://arxiv.org/abs/1603.02754)) no está implementado en scikit-learn, sin embargo existe en el paquete [xgboost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html), que proporciona estimadores en base a dicho algoritmo que son compatibles con sklearn.\n",
    "\n",
    "Podemos instalar `xgboost` de conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/21/8b2ec99862903a6d3aed62ce156d21d114b8666e669c46d9e54041df9496/xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl (16.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.6MB 213kB/s ta 0:00:01   22% |███████▎                        | 3.8MB 210kB/s eta 0:01:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from xgboost) (1.15.0)\n",
      "Requirement already satisfied: scipy in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from xgboost) (1.1.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.81\n"
     ]
    }
   ],
   "source": [
    "#!conda install -y -c conda-forge xgboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    n_estimators : int\n",
      "        Number of boosted trees to fit.\n",
      "    silent : boolean\n",
      "        Whether to print messages while running boosting.\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: string\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    nthread : int\n",
      "        Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "    min_child_weight : int\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : int\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each split, in each level.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    seed : int\n",
      "        Random number seed.  (Deprecated, please use random_state)\n",
      "    random_state : int\n",
      "        Random number seed.  (replaces seed)\n",
      "    missing : float, optional\n",
      "        Value in the data which needs to be present as a missing value. If\n",
      "        None, defaults to np.nan.\n",
      "    \\*\\*kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "        be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "        will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "            passed via this argument will interact properly with scikit-learn.\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "    A custom objective function can be provided for the ``objective``\n",
      "    parameter. In this case, it should have the signature\n",
      "    ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "    y_true: array_like of shape [n_samples]\n",
      "        The target values\n",
      "    y_pred: array_like of shape [n_samples]\n",
      "        The predicted values\n",
      "\n",
      "    grad: array_like of shape [n_samples]\n",
      "        The value of the gradient for each sample point.\n",
      "    hess: array_like of shape [n_samples]\n",
      "        The value of the second derivative for each sample point\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(XGBRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9976500188758033"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_xgboost = XGBRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_xgboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"xgboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import plot_importance, to_graphviz\n",
    "estimador_xgboost.fit(boston[datos.feature_names], boston.objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos un modelo entrenado, podemos ver la importancia de las variables en partir los árboles mediante el método `plot_importance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1ca5450390>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(estimador_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `to_graphviz` imprime un árbol en concreto (pasandole el parámetro `rankdir='LR'` lo imprime en horizontal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"946pt\" height=\"710pt\"\n",
       " viewBox=\"0.00 0.00 946.26 710.29\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 706.286)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-706.286 942.256,-706.286 942.256,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"67.594\" cy=\"-370.142\" rx=\"67.6881\" ry=\"67.6881\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.594\" y=\"-366.442\" font-family=\"Times,serif\" font-size=\"14.00\">RM&lt;6.5454998</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"325.33\" cy=\"-445.142\" rx=\"85.2851\" ry=\"85.2851\"/>\n",
       "<text text-anchor=\"middle\" x=\"325.33\" y=\"-441.442\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT&lt;19.6450005</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M132.797,-388.971C163.231,-397.897 200.125,-408.717 233.377,-418.468\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"232.527,-421.867 243.108,-421.322 234.497,-415.149 232.527,-421.867\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.688\" y=\"-417.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"325.33\" cy=\"-260.142\" rx=\"81.4863\" ry=\"81.4863\"/>\n",
       "<text text-anchor=\"middle\" x=\"325.33\" y=\"-256.442\" font-family=\"Times,serif\" font-size=\"14.00\">NOX&lt;0.659000039</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M129.849,-343.795C163.083,-329.5 204.645,-311.623 240.807,-296.068\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"242.559,-299.125 250.362,-291.958 239.793,-292.695 242.559,-299.125\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.688\" y=\"-334.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"610.364\" cy=\"-630.142\" rx=\"72.2875\" ry=\"72.2875\"/>\n",
       "<text text-anchor=\"middle\" x=\"610.364\" y=\"-626.442\" font-family=\"Times,serif\" font-size=\"14.00\">DIS&lt;1.37275004</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M397.022,-491.349C441.077,-520.144 497.47,-557.005 541.068,-585.502\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"539.324,-588.543 549.609,-591.084 543.154,-582.683 539.324,-588.543\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.973\" y=\"-557.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"610.364\" cy=\"-445.142\" rx=\"94.7833\" ry=\"94.7833\"/>\n",
       "<text text-anchor=\"middle\" x=\"610.364\" y=\"-441.442\" font-family=\"Times,serif\" font-size=\"14.00\">PTRATIO&lt;19.6500015</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M410.484,-445.142C440.259,-445.142 474.112,-445.142 505.34,-445.142\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"505.379,-448.643 515.379,-445.142 505.379,-441.643 505.379,-448.643\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.973\" y=\"-448.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node10\" class=\"node\"><title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"610.364\" cy=\"-260.142\" rx=\"72.2875\" ry=\"72.2875\"/>\n",
       "<text text-anchor=\"middle\" x=\"610.364\" y=\"-256.442\" font-family=\"Times,serif\" font-size=\"14.00\">RM&lt;7.43700027</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M406.68,-260.142C444.636,-260.142 489.862,-260.142 527.953,-260.142\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"527.983,-263.643 537.983,-260.142 527.983,-256.643 527.983,-263.643\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.973\" y=\"-263.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node11\" class=\"node\"><title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"610.364\" cy=\"-85.1424\" rx=\"85.2851\" ry=\"85.2851\"/>\n",
       "<text text-anchor=\"middle\" x=\"610.364\" y=\"-81.4424\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT&lt;24.5300007</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M394.845,-217.78C435.252,-192.796 486.594,-161.051 528.919,-134.882\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"530.867,-137.793 537.531,-129.557 527.185,-131.839 530.867,-137.793\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.973\" y=\"-196.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"929.256,-675.142 819.256,-675.142 819.256,-639.142 929.256,-639.142 929.256,-675.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-653.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.62421072</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M682.236,-637.449C721.729,-641.521 770.529,-646.552 808.919,-650.51\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"808.641,-653.999 818.947,-651.543 809.359,-647.036 808.641,-653.999\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-651.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"932.256,-621.142 816.256,-621.142 816.256,-585.142 932.256,-585.142 932.256,-621.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-599.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.602076232</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M682.236,-622.836C720.591,-618.882 767.726,-614.022 805.577,-610.12\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"806.403,-613.553 815.991,-609.046 805.685,-606.59 806.403,-613.553\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-621.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\"><title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"932.256,-490.142 816.256,-490.142 816.256,-454.142 932.256,-454.142 932.256,-490.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-468.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.538198948</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M704.804,-454.776C738.195,-458.218 775.139,-462.027 805.9,-465.198\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"805.774,-468.704 816.081,-466.248 806.492,-461.741 805.774,-468.704\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-466.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\"><title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"929.256,-436.142 819.256,-436.142 819.256,-400.142 929.256,-400.142 929.256,-436.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-414.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.22415939</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M704.804,-435.509C739.25,-431.958 777.478,-428.017 808.799,-424.788\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"809.552,-428.229 819.14,-423.722 808.834,-421.266 809.552,-428.229\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-436.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"925.756,-305.142 822.756,-305.142 822.756,-269.142 925.756,-269.142 925.756,-305.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-283.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.0034579</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M682.236,-267.449C722.98,-271.65 773.631,-276.872 812.537,-280.883\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"812.364,-284.383 822.67,-281.927 813.082,-277.42 812.364,-284.383\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-281.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"929.256,-251.142 819.256,-251.142 819.256,-215.142 929.256,-215.142 929.256,-251.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-229.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.51664674</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>5&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M682.236,-252.836C721.729,-248.764 770.529,-243.733 808.919,-239.775\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"809.359,-243.249 818.947,-238.741 808.641,-236.286 809.359,-243.249\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-251.942\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"932.256,-130.142 816.256,-130.142 816.256,-94.1424 932.256,-94.1424 932.256,-130.142\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-108.442\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.383431166</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M695.309,-93.7968C731.104,-97.4872 772.302,-101.734 806.037,-105.212\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"805.947,-108.722 816.253,-106.266 806.665,-101.758 805.947,-108.722\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-106.942\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"938.256,-76.1424 810.256,-76.1424 810.256,-40.1424 938.256,-40.1424 938.256,-76.1424\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.256\" y=\"-54.4424\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=&#45;0.0193481203</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>6&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M695.309,-76.4881C728.925,-73.0224 767.306,-69.0655 799.795,-65.716\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"800.535,-69.1583 810.124,-64.6512 799.817,-62.1952 800.535,-69.1583\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.756\" y=\"-76.9424\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f1ca6371c50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_graphviz(estimador_xgboost, num_trees=11, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el output de un árbol no está en la misma escala que las predicciones (la variable objetivo tiene el rango 5-50), esto es así por que en el algoritmo XGBoost cada árbol se basa en el output del árbol anterior, intentando corregir el error producido por el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de stacking simplemente usa el output (generalmente en terminos de probabilidades para casos de clasificacion o de las predicciones en casos de regresión) de múltiples modelos como input para un nuevo *metamodelo*.\n",
    "\n",
    "scikit learn no tiene un estimador de stacking por defecto, sin embargo, podemos usar el  estimador de stacking (`StackingRegressor`) de [mlxtend](https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/), una librería que amplia las funcionalidades de `sklearn`\n",
    "\n",
    "Podemos instalar mlxtend asi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/d1/1b9e85e991f836e9aaea18367ff628a6324af1005971dc9f57e51a2ab5a4/mlxtend-0.14.0-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 318kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (0.19.1)\n",
      "Requirement already satisfied: setuptools in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (40.6.3)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: pandas>=0.17.1 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (0.23.4)\n",
      "Requirement already satisfied: scipy>=0.17 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from mlxtend) (1.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (2.7.5)\n",
      "Requirement already satisfied: pytz in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (2018.7)\n",
      "Requirement already satisfied: six>=1.10 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (1.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/erdvillegas/anaconda3/envs/Datascientist/lib/python3.7/site-packages (from matplotlib>=1.5.1->mlxtend) (1.0.1)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Stacking regressor for scikit-learn estimators for regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    regressors : array-like, shape = [n_regressors]\n",
      "        A list of regressors.\n",
      "        Invoking the `fit` method on the `StackingRegressor` will fit clones\n",
      "        of those original regressors that will\n",
      "        be stored in the class attribute\n",
      "        `self.regr_`.\n",
      "    meta_regressor : object\n",
      "        The meta-regressor to be fitted on the ensemble of\n",
      "        regressors\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "        - `verbose=0` (default): Prints nothing\n",
      "        - `verbose=1`: Prints the number & name of the regressor being fitted\n",
      "        - `verbose=2`: Prints info about the parameters of the\n",
      "                       regressor being fitted\n",
      "        - `verbose>2`: Changes `verbose` param of the underlying regressor to\n",
      "           self.verbose - 2\n",
      "    use_features_in_secondary : bool (default: False)\n",
      "        If True, the meta-regressor will be trained both on\n",
      "        the predictions of the original regressors and the\n",
      "        original dataset.\n",
      "        If False, the meta-regressor will be trained only on\n",
      "        the predictions of the original regressors.\n",
      "    store_train_meta_features : bool (default: False)\n",
      "        If True, the meta-features computed from the training data\n",
      "        used for fitting the\n",
      "        meta-regressor stored in the `self.train_meta_features_` array,\n",
      "        which can be\n",
      "        accessed after calling `fit`.\n",
      "\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    regr_ : list, shape=[n_regressors]\n",
      "        Fitted regressors (clones of the original regressors)\n",
      "    meta_regr_ : estimator\n",
      "        Fitted meta-regressor (clone of the original meta-estimator)\n",
      "    coef_ : array-like, shape = [n_features]\n",
      "        Model coefficients of the fitted meta-estimator\n",
      "    intercept_ : float\n",
      "        Intercept of the fitted meta-estimator\n",
      "    train_meta_features : numpy array, shape = [n_samples, len(self.regressors)]\n",
      "        meta-features for training data, where n_samples is the\n",
      "        number of samples\n",
      "        in training data and len(self.regressors) is the number of regressors.\n",
      "    refit : bool (default: True)\n",
      "        Clones the regressors for stacking regression if True (default)\n",
      "        or else uses the original ones, which will be refitted on the dataset\n",
      "        upon calling the `fit` method. Setting refit=False is\n",
      "        recommended if you are working with estimators that are supporting\n",
      "        the scikit-learn fit/predict API interface but are not compatible\n",
      "        to scikit-learn's `clone` function.\n",
      "\n",
      "    Examples\n",
      "    -----------\n",
      "    For usage examples, please see\n",
      "    http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(StackingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos usar los estimadores ensamblados que hemos creado en este notebook para crear un nuevo estimador. Dicho estimador no tiene garantizado un funcionamiento mejor que el mejor de los estimadores que usa como input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.131842423981489"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_stacking = StackingRegressor(\n",
    "    regressors=[\n",
    "        BaggingRegressor(n_estimators=100),\n",
    "        AdaBoostRegressor(n_estimators=100),\n",
    "        GradientBoostingRegressor(n_estimators=100),\n",
    "        RandomForestRegressor(n_estimators=100)\n",
    "    ], \n",
    "    meta_regressor=XGBRegressor(n_estimators=100))\n",
    "\n",
    "\n",
    "error_cv = cross_val_score(estimador_stacking, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"stacking\"] = error_cv\n",
    "\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adaboost_100',\n",
       " 'arbol',\n",
       " 'bagging_arbol_10',\n",
       " 'bagging_arbol_100',\n",
       " 'bagging_elnet',\n",
       " 'bagging_extra_arbol',\n",
       " 'elasticnet',\n",
       " 'gradientboost_100',\n",
       " 'lasso',\n",
       " 'randomforest_100',\n",
       " 'ridge',\n",
       " 'stacking',\n",
       " 'xgboost_100']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
